---
title: "Protein data analysis"
author: "pratik"
date: "16/02/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
###1 Introduction
##k-means is an unsupervised machine learning algorithm used to find groups of observations (clusters) that share similar characteristics. What is the meaning of unsupervised learning? It means that the observations given in the data set are unlabeled, there is no outcome to be predicted. We are going to use a Protein data set to cluster different types of wines. This data set contains the results of a chemical analysis of wines grown in a specific area of 26 countries.


## Lets clean the unnecessary items
##2 Loading data
#First we need to load some libraries and read the data set.
```{r}
gc()
rm(list = ls(all = TRUE))
```


```{r}
packages<-function(x){
  x<-as.character(match.call()[[2]])
  if (!require(x,character.only=TRUE)){
    install.packages(pkgs=x,repos="http://cran.r-project.org")
    require(x,character.only=TRUE)
  }
}
```

```{r}
packages(tidyverse) # data manipulation
packages(corrplot)
packages(gridExtra)
packages(GGally)
packages(cluster) # clustering algorithms 
packages(factoextra) # clustering algorithms & visualization

```



```{r}
library(tidyverse)
library(corrplot)
library(gridExtra)
library(GGally)
library(knitr)
library(NbClust)
```


```{r}
Protein<-read.csv("C:\\Users\\prati\\Desktop\\data\\Protein.csv")
Protein

```

##We don’t need the Customer_Segment column. As we have said before, k-means is an unsupervised machine learning algorithm and works with unlabeled data.
# Remove the Type column
```{r}
Protein <- Protein[, -1]#first row 
Protein
```

```{r}
kable(head(Protein))
kable(tail( Protein))
kable(summary(Protein))
str(Protein)
dim(Protein)
colnames(Protein)
```

###3 Data analysis

# Histogram for each Attribute
```{r}
Protein %>%
  gather(Attributes, value,1:9) %>%
  ggplot(aes(x=value, fill=Attributes)) +
  geom_histogram(colour="black", show.legend=FALSE) +
  facet_wrap(~Attributes, scales="free_x") +
  labs(x="Values", y="Frequency",
       title="Protein Attributes -Histograms") +
  theme_bw()
```


```{r}

# Density plot for each Attribute
Protein %>%
  gather(Attributes, value, 1:9) %>%
  ggplot(aes(x=value, fill=Attributes)) +
  geom_density(colour="black", alpha=0.5, show.legend=FALSE) +
  facet_wrap(~Attributes, scales="free_x") +
  labs(x="Values", y="Density",
       title="Wines Attributes - Density plots") +
  theme_bw()


```


```{r}

# Boxplot for each Attribute  
Protein %>%
  gather(Attributes, values, c(1:9, 1:7)) %>%
  ggplot(aes(x=reorder(Attributes, values, FUN=median), y=values, fill=Attributes)) +
  geom_boxplot(show.legend=FALSE) +
  labs(title="Wines Attributes - Boxplots") +
  theme_bw() +
  theme(axis.title.y=element_blank(),
        axis.title.x=element_blank()) +
  ylim(0, 35) +
  coord_flip()

```

# Correlation matrix 
```{r}
corrplot(cor(Protein), type="upper", method="number", tl.cex=1)     ###method = eliipse
```
##There is a strong linear correlation between Fish and Milk. We can model the relationship between these two variables by fitting a linear equation



# Relationship between WhiteMeat and Milk
```{r}

ggplot(Protein, aes(x=WhiteMeat, y=Milk)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE) +
  labs(title="Protein Attributes",
       subtitle="Relationship between WhiteMeat and Milk") +
  theme_bw()
```
#let’s prepare our data to do the K means clustering

#From the data summary, we have seen that there are variables who are on a different scale, we need to either scale the data or normalise it. We can normalise the data using the mean and standard deviation, also we can use scale function to normalise our data.




# Relationship between Eggs and Milk
```{r}
ggplot(Protein, aes(x=Eggs, y=Milk)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE) +
  labs(title="Protein Attributes",
       subtitle="Relationship between Eggs and Milk") +
  theme_bw()
```





# Relationship between Fish and Milk
```{r}
ggplot(Protein, aes(x=Fish, y=Milk)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE) +
  labs(title="Protein Attributes",
       subtitle="Relationship between Fish and Milk") +
  theme_bw()
```


# Relationship between Milk and Cereals
```{r}
ggplot(Protein, aes(x=Milk, y=Cereals)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE) +
  labs(title="Protein Attributes",
       subtitle="Relationship between Milk and Cereals") +
  theme_bw()

```




# Normalization
```{r}
ProteinNorm <- as.data.frame(scale(Protein))
ProteinNorm
```
# Original data
```{r}
p1 <- ggplot(Protein, aes(x=Milk, y=Eggs)) +
  geom_point() +
  labs(title="Original data") +
  theme_bw()

```
# Normalized data 
```{r}
p2 <- ggplot(ProteinNorm, aes(x=Milk, y=Eggs)) +
  geom_point() +
  labs(title="Normalized data") +
  theme_bw()
```

# Subplot

```{r}
grid.arrange(p1, p2, ncol=2)
```

##The points in the normalized data are the same as the original one. The only thing that changes is the scale of the axis.


##5 k-means execution

# Execution of k-means with k=2
```{r}
set.seed(1234)
Protein_k2 <- kmeans(ProteinNorm, centers=2)
fviz_cluster(Protein_k2, data = ProteinNorm)

```
#When we print the model we build (wines_k2), it shows information like, number of clusters, centers of the clusters, size of the clusters and sum of square. Let’s check how to get these attributes of our model.

# Cluster to which each point is allocated
```{r}
Protein_k2$cluster
```
# Cluster centers
```{r}
Protein_k2$centers

```
# Cluster size
```{r}
Protein_k2$size
```
# Between-cluster sum of squares
```{r}
Protein_k2$betweenss
```

# Within-cluster sum of squares
```{r}
Protein_k2$withinss
```
# Total within-cluster sum of squares 
```{r}
Protein_k2$tot.withinss

```
# Total sum of squares
```{r}

Protein_k2$totss

```

```{r} 
Protein_K3 <- kmeans(ProteinNorm, centers = 3, nstart = 25)
Protein_K4 <- kmeans(ProteinNorm, centers = 4, nstart = 25)
Protein_K5 <- kmeans(ProteinNorm, centers = 5, nstart = 25)

```

```{r}
p1 <- fviz_cluster(Protein_k2, geom = "point", data = ProteinNorm) + ggtitle(" K = 2")
p2 <- fviz_cluster(Protein_K3, geom = "point", data = ProteinNorm) + ggtitle(" K = 3")
p3 <- fviz_cluster(Protein_K4, geom = "point", data = ProteinNorm) + ggtitle(" K = 4")
p4 <- fviz_cluster(Protein_K5, geom = "point", data = ProteinNorm) + ggtitle(" K = 5")

grid.arrange(p1, p2, p3, p4, nrow = 2)
```
##Determining Optimal Clusters
#K-means clustering requires that you specify in advance the number of clusters to extract. A plot of the total Protein-groups sums of squares against the number of clusters in a k-means solution can be helpful. A bend in the graph can suggest the appropriate number of clusters.

#Below are the methods to determine the optimal number of clusters

#1.Elbow method
#2.Silhouette method
#3.Gap statistic
# Determining Optimal clusters (k) Using Elbow method
```{r}
fviz_nbclust(x = ProteinNorm,FUNcluster = kmeans, method = 'wss' )
```
#The above one line code work better to find the number of clusters using Elbow method, however, we can do the same thing by making a function which takes your data (ProteinNorm) as an input. Let’s see the below line of code which is used as a function to create a plot to find number of clusters.

```{r}
wssplot <- function(data, nc = 15, set.seed = 1234){
  wss <- (nrow(data) - 1)*sum(apply(data, 2, var))
  for(i in 2:nc) {
    set.seed(1234)
    wss[i] <- sum(kmeans(x = data, centers = i, nstart = 25)$withinss)
  }
  plot(1:nc, wss, type = 'b', xlab = 'Number of Clusters', ylab = 'Within Group Sum of Square',
       main = 'Elbow Method Plot to Find Optimal Number of Clusters', frame.plot = T,
       col = 'blue', lwd = 1.5)
}

wssplot(ProteinNorm)

```
##Fortunately, this process to compute the “Elbow method” has been wrapped up in a single function (fviz_nbclust):

# Determining Optimal clusters (k) Using Average Silhouette Method
```{r}

fviz_nbclust(x = ProteinNorm,FUNcluster = kmeans, method = 'silhouette' )

```
#There is another method called Gap-Static used for finding the optimal value of K.


# compute gap statistic
```{r}
set.seed(123)
gap_stat <- clusGap(x = ProteinNorm, FUN = kmeans, K.max = 15, nstart = 25, B = 50 )

# Print the result
print(gap_stat, method = "firstmax")
```

# plot the result to determine the optimal number of clusters.
```{r}
fviz_gap_stat(gap_stat)
```
With most of these approaches suggesting 2 as the number of optimal clusters, we can perform the final analysis and extract the results using 2 clusters.


# Compute k-means clustering with k = 3
```{r}
set.seed(123)
final <- kmeans(ProteinNorm, centers = 3, nstart = 25)
print(final)
```

```{r}

fviz_cluster(final, data = ProteinNorm)

```
#We can extract the clusters and add to our initial data to do some descriptive statistics at the cluster level

```{r}
ProteinNorm %>% 
  mutate(Cluster = final$cluster) %>%
  group_by(Cluster) %>%
  summarize_all('median')
```
##Summary
#K Means clustering is a simple algorithm used to partition n observations into k clusters in which each observation is belongs to the cluster with the nearest mean.

#So far we have learned:

#prepare the data for cluster analysis (for K-Means cluster). Use the numerical variables and normalizing the data is recommended.



###6 clusters
###To study graphically which value of k gives us the best partition, we can plot betweenss and tot.withinss vs Choice of k.
```{r}
bss <- numeric()
wss <- numeric()
```

# Run the algorithm for different values of k 
```{r}

set.seed(1234)

for(i in 1:10){

  # For each k, calculate betweenss and tot.withinss
  bss[i] <- kmeans(ProteinNorm, centers=i)$betweenss
  wss[i] <- kmeans(ProteinNorm, centers=i)$tot.withinss

}
```



# Between-cluster sum of squares vs Choice of k

```{r}
p_3 <- qplot(1:10, bss, geom=c("point", "line"), 
            xlab="Number of clusters", ylab="Between-cluster sum of squares") +
  scale_x_continuous(breaks=seq(0, 10, 1)) +
  theme_bw()
```
# Total within-cluster sum of squares vs Choice of k
```{r}
p_4 <- qplot(1:10, wss, geom=c("point", "line"),
            xlab="Number of clusters", ylab="Total within-cluster sum of squares") +
  scale_x_continuous(breaks=seq(0, 10, 1)) +
  theme_bw()
```

```{r}
# Subplot
grid.arrange(p_3, p_4, ncol=2)

```

##Which is the optimal value for k? One should choose a number of clusters so that adding another cluster doesn’t give much better partition of the data. At some point the gain will drop, giving an angle in the graph (elbow criterion). The number of clusters is chosen at this point. In our case, it is clear that 3 is the appropriate value for k.




##7 Results
# Execution of k-means with k=3

```{r}
set.seed(1234)

Protein_k3 <- kmeans(Protein, centers=3)
```
# Mean values of each cluster
```{r}
aggregate(Protein, by=list(Protein_k3$cluster), mean)




```

# Clustering 
```{r}
ggpairs(cbind(Protein, Cluster=as.factor(Protein_k3$cluster)),
        columns=1:6, aes(colour=Cluster, alpha=0.5),
        lower=list(continuous="points"),
        upper=list(continuous="blank"),
        axisLabels="none", switch="both") +
        theme_bw()
```


```{r}
# Compute k-means clustering with k = 3
set.seed(123)
final <- kmeans(ProteinNorm, centers = 3, nstart = 25)
print(final)

```

```{r}

fviz_cluster(final, data = ProteinNorm)

```
##We can extract the clusters and add to our initial data to do some descriptive statistics at the cluster level


##8 Summary
##In this entry we have learned about the k-means algorithm, including the data normalization before we execute it, the choice of the optimal number of clusters (elbow criterion) and the visualization of the clustering.

##It has been a pleasure to make this post, I have learned a lot! Thank you for reading and if you like it, please upvote it.






# Calculating Distances
```{r}

head(Protein, 2)
d <- dist(Protein)
as.matrix(d)[1:5,1:5]

```
```{r}
head(Protein)
```






```{r}
# Average linkage clustering of milk data
Protein.scaled <- scale(Protein[,-1])                                  
d <- dist(Protein.scaled)                                          
fit.average <- hclust(d, method="average")                          
plot(fit.average, main="Average Linkage Clustering")
rect.hclust(fit.average, k=6)
```

```{r}
heatmap(Protein.scaled)
```
# Selecting the number of clusters


```{r}
head(Protein.scaled)
```

```{r}
nc <- NbClust(Protein.scaled, distance="euclidean", 
              min.nc=9, max.nc=15, method="average")
```

```{r}
par(mfrow=c(1,1))
```

```{r}
nc$Best.nc
nc$Best.nc[1,]

table(nc$Best.nc[1,])
barplot(table(nc$Best.nc[1,]), 
        xlab="Numer of Clusters", ylab="Number of Criteria",
        main="Number of Clusters Chosen by 26 Criteria") 

```

```{r}
# Obtaining the final cluster solution
clusterID <- cutree(fit.average, k=3)
```

```{r}
# Assigning ClusterID to observations
MilkClust <- data.frame(Protein , clusterID)
```

```{r}
plot(fit.average,main="Average Linkage Clustering\n3 Cluster Solution")
rect.hclust(fit.average, k=3)
```

# Kmeans
```{r}
k4<-kmeans(Protein.scaled, centers=4)
```

```{r}
# Plot function for within groups sum of squares by number of clusters
wssplot <- function(data, nc=15, seed=1234) {
  wss <- array(dim=c(nc))
  for (i in 2:nc){
    set.seed(seed)
    km <- kmeans(data, centers=i)
    wss[i] <- km$tot.withinss
    }
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")
}
```


```{r}
wssplot(Protein.scaled) 

k5<-kmeans(Protein.scaled, centers=5)

```


```{r}
k5clust <- data.frame(Protein , k5$cluster)

pairs(k5clust[,-6],col= k5$cluster)

```



